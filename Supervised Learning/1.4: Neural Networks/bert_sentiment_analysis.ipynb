{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyBOMScqkmsT"
      },
      "source": [
        "# Large Language Models (LLMs)\n",
        "Large language models (LLMs) are a type of neural network model. They utilize deep learning techniques, specifically architectures like transformers, to understand and generate human-like text at scale. Here's how they relate to neural networks:\n",
        "\n",
        "## Architecture:\n",
        "LLMs are built using neural network architectures, typically based on transformers. Transformers consist of multiple layers of neural network components, including self-attention mechanisms and feed-forward neural networks, that process sequential data such as text.\n",
        "\n",
        "## Training:\n",
        "LLMs are trained using deep learning techniques, which involve optimizing the parameters of the neural network model to minimize a loss function. During training, the model learns the statistical patterns and structures of natural language by processing vast amounts of text data.\n",
        "Representation Learning: LLMs learn to represent and encode textual information in distributed vector representations, often referred to as embeddings. These embeddings capture semantic and syntactic information about words and phrases in the input text, enabling the model to understand and generate coherent text.\n",
        "\n",
        "## Fine-tuning:\n",
        "Similar to other neural network models, LLMs can be fine-tuned on specific tasks or domains to improve their performance. Fine-tuning involves updating the parameters of the pre-trained LLM on a smaller dataset relevant to the target task, allowing the model to adapt its representations and predictions accordingly.\n",
        "Scalability: One of the key advantages of LLMs is their scalability, which is achieved through parallelization and distributed computing techniques. LLMs can be trained on massive datasets using distributed training across multiple GPUs or even multiple machines, allowing them to capture complex patterns in natural language effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-KtILJoiwMl"
      },
      "source": [
        "Here I will show how to fine-tune Google's BERT for sentiment analysis. This will be implemented on the IMDB Movie Review Dataset, with binary classifications: positive and negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhBP47GRUD_w",
        "outputId": "b024b3d1-59b7-4e6e-c883-d56825b944b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers numpy torch scikit-learn\n",
        "!pip install accelerate>=0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OwAkTJcAJnkG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers.file_utils import is_tf_available, is_torch_available\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EjMjPlsEXttI"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sFv-FiYtKuuf"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2bsCS5BjLc4"
      },
      "source": [
        "We set a seed to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gDRivaE1KYWA"
      },
      "outputs": [],
      "source": [
        "# the model we gonna train, base uncased BERT\n",
        "# check text classification models here: https://huggingface.co/models?filter=text-classification\n",
        "model_name = \"bert-base-uncased\"\n",
        "# max sequence length for each document/sentence sample\n",
        "max_length = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wJnm4MOjSzc"
      },
      "source": [
        "BERT's tokenizer has a max_length of 512, so any review beyond 512 tokens will be truncated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWtlkkVKXHc0",
        "outputId": "4a7738a9-d822-4450-8431-a5f81f5f9254"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67bdT1b4Tyrb",
        "outputId": "ded8fad7-f8eb-4651-ddf0-bdcc0e45c996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting IMDb dataset...\n",
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "def download_and_extract_imdb(data_dir='aclImdb'):\n",
        "    \"\"\"Download and extract the IMDb dataset.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to store the dataset.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "    dataset_path = os.path.join(data_dir, \"aclImdb_v1.tar.gz\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "\n",
        "    # Download the dataset\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(\"Downloading IMDb dataset...\")\n",
        "        urllib.request.urlretrieve(url, dataset_path)\n",
        "        print(\"Download completed.\")\n",
        "\n",
        "    # Extract the dataset\n",
        "    print(\"Extracting IMDb dataset...\")\n",
        "    with tarfile.open(dataset_path, 'r:gz') as tar:\n",
        "        tar.extractall(data_dir)\n",
        "    print(\"Extraction completed.\")\n",
        "\n",
        "# Download and extract IMDb dataset\n",
        "download_and_extract_imdb()\n",
        "\n",
        "# Now you can proceed to use the previously provided code to load and preprocess the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhw9oe-QjfXH"
      },
      "source": [
        "Here we import the IMDB dataset, and we will split it up into train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSanWgrgXhbG",
        "outputId": "844515b6-4d51-4bd8-8281-8db1720bb070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training size: 20000\n",
            "Validation size: 5000\n",
            "Testing size: 25000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_imdb_data(data_dir='aclImdb'):\n",
        "    \"\"\"Read IMDb data from the provided directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to the IMDb dataset directory.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple of training texts, training labels, testing texts, testing labels.\n",
        "    \"\"\"\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "\n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(data_dir, data_dir, 'train', category)\n",
        "        test_path = os.path.join(data_dir, data_dir, 'test', category)\n",
        "\n",
        "        # Read training data\n",
        "        for fname in os.listdir(train_path):\n",
        "            with open(os.path.join(train_path, fname), 'r', encoding='utf-8') as f:\n",
        "                train_texts.append(f.read())\n",
        "                train_labels.append(1 if category == 'pos' else 0)\n",
        "\n",
        "        # Read testing data\n",
        "        for fname in os.listdir(test_path):\n",
        "            with open(os.path.join(test_path, fname), 'r', encoding='utf-8') as f:\n",
        "                test_texts.append(f.read())\n",
        "                test_labels.append(1 if category == 'pos' else 0)\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "# Load IMDb dataset\n",
        "train_texts, train_labels, test_texts, test_labels = read_imdb_data()\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n",
        "\n",
        "# Print size of the datasets\n",
        "print(f\"Training size: {len(train_texts)}\")\n",
        "print(f\"Validation size: {len(valid_texts)}\")\n",
        "print(f\"Testing size: {len(test_texts)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E_r0ECJaXO-y"
      },
      "outputs": [],
      "source": [
        "# tokenize the dataset, truncate when passed `max_length`,\n",
        "# and pad with 0's when less than `max_length`\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7riiOpJiXdTd"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = IMDBDataset(train_encodings, train_labels)\n",
        "valid_dataset = IMDBDataset(valid_encodings, valid_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4aAwDGZXnyk",
        "outputId": "58ecd52b-c306-4724-f825-872c57722eaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Define target names based on your dataset\n",
        "target_names = [\"negative\", \"positive\"]\n",
        "\n",
        "# load the model and pass to CUDA\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names)).to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AkZN1X-zOoe0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBEtRIsGjrns"
      },
      "source": [
        "We will be judging our model's performance through accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MjMB9f01Pyiu"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=400,               # log & save weights each logging_steps\n",
        "    save_steps=400,\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BukYQXs2P35S"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "s5a7QY_wP5iD",
        "outputId": "11ed8c2f-87ad-49c0-b966-5c9ec9e0d5a7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='455' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 455/2500 08:34 < 38:40, 0.88 it/s, Epoch 0.18/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.443800</td>\n",
              "      <td>0.342545</td>\n",
              "      <td>0.898000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 49:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.443800</td>\n",
              "      <td>0.342545</td>\n",
              "      <td>0.898000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.377600</td>\n",
              "      <td>0.271921</td>\n",
              "      <td>0.911800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.340900</td>\n",
              "      <td>0.354372</td>\n",
              "      <td>0.901600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.294300</td>\n",
              "      <td>0.297910</td>\n",
              "      <td>0.915000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.298500</td>\n",
              "      <td>0.317626</td>\n",
              "      <td>0.909000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.232600</td>\n",
              "      <td>0.277936</td>\n",
              "      <td>0.926200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.3277172790527344, metrics={'train_runtime': 2941.8441, 'train_samples_per_second': 6.798, 'train_steps_per_second': 0.85, 'total_flos': 5262221107200000.0, 'train_loss': 0.3277172790527344, 'epoch': 1.0})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErJaHWaIjyEQ"
      },
      "source": [
        "We can see the slow (not always increasing) progression of accuracy with each step.\n",
        "\n",
        "Side note: this is an extremely computationally expensive process, and takes close to an hour to fully train. That's why I limited it to only 1 epoch, when it is traditionally 3.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "cdWlnZxAR0XA",
        "outputId": "42d8cf56-4e61-4234-df89-94377a7c4de0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 02:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.2719208598136902,\n",
              " 'eval_accuracy': 0.9118,\n",
              " 'eval_runtime': 164.0067,\n",
              " 'eval_samples_per_second': 30.487,\n",
              " 'eval_steps_per_second': 1.524,\n",
              " 'epoch': 1.0}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate the current model after training\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhGoby34k4g5"
      },
      "source": [
        "Our models ended with a test accuracy of 91.18% Great success! Onto the next ML model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
